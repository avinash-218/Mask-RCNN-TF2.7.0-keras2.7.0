{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mrcnn\n",
    "import mrcnn.config\n",
    "import mrcnn.model\n",
    "import mrcnn.visualize\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the class label names from disk, one label per line\n",
    "# CLASS_NAMES = open(\"coco_labels.txt\").read().strip().split(\"\\n\")\n",
    "\n",
    "CLASS_NAMES = ['BG', 'armchair', 'bed', 'door1', 'door2', 'sink1', 'sink2', 'sink3', 'sink4', 'sofa1', 'sofa2', 'table1', 'table2', 'table3', 'tub', 'window1', 'window2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22537149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConfig(mrcnn.config.Config):\n",
    "    \"\"\"Base configuration class. For custom configurations, create a\n",
    "    sub-class that inherits from this one and override properties\n",
    "    that need to be changed.\n",
    "    \"\"\"\n",
    "    # Name the configurations. For example, 'COCO', 'Experiment 3', ...etc.\n",
    "    # Useful if your code needs to do things differently depending on which\n",
    "    # experiment is running.\n",
    "    NAME = 'furnitures'  # Override in sub-classes\n",
    "\n",
    "    # NUMBER OF GPUs to use. When using only a CPU, this needs to be set to 1.\n",
    "    GPU_COUNT = 1\n",
    "\n",
    "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
    "    # handle 2 images of 1024x1024px.\n",
    "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
    "    # number that your GPU can handle for best performance.\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    # This doesn't need to match the size of the training set. Tensorboard\n",
    "    # updates are saved at the end of each epoch, so setting this to a\n",
    "    # smaller number means getting more frequent TensorBoard updates.\n",
    "    # Validation stats are also calculated at each epoch end and they\n",
    "    # might take a while, so don't set this too small to avoid spending\n",
    "    # a lot of time on validation stats.\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # Number of validation steps to run at the end of every training epoch.\n",
    "    # A bigger number improves accuracy of validation stats, but slows\n",
    "    # down the training.\n",
    "    VALIDATION_STEPS = 20\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101.\n",
    "    # You can also provide a callable that should have the signature\n",
    "    # of model.resnet_graph. If you do so, you need to supply a callable\n",
    "    # to COMPUTE_BACKBONE_SHAPE as well\n",
    "    BACKBONE = \"resnet101\"\n",
    "\n",
    "    # Only useful if you supply a callable to BACKBONE. Should compute\n",
    "    # the shape of each layer of the FPN Pyramid.\n",
    "    # See model.compute_backbone_shapes\n",
    "    COMPUTE_BACKBONE_SHAPE = None\n",
    "\n",
    "    # The strides of each layer of the FPN Pyramid. These values\n",
    "    # are based on a Resnet101 backbone.\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "\n",
    "    # Size of the fully-connected layers in the classification graph\n",
    "    FPN_CLASSIF_FC_LAYERS_SIZE = 1024\n",
    "\n",
    "    # Size of the top-down layers used to build the feature pyramid\n",
    "    TOP_DOWN_PYRAMID_SIZE = 256\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 1 + 16 # Override in sub-classes\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\n",
    "\n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "\n",
    "    # Anchor stride\n",
    "    # If 1 then anchors are created for each cell in the backbone feature map.\n",
    "    # If 2, then anchors are created for every other cell, and so on.\n",
    "    RPN_ANCHOR_STRIDE = 1\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.7\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 256\n",
    "    \n",
    "    # ROIs kept after tf.nn.top_k and before non-maximum suppression\n",
    "    PRE_NMS_LIMIT = 6000\n",
    "\n",
    "    # ROIs kept after non-maximum suppression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 2000\n",
    "    POST_NMS_ROIS_INFERENCE = 1000\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = False\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Input image resizing\n",
    "    # Generally, use the \"square\" resizing mode for training and predicting\n",
    "    # and it should work well in most cases. In this mode, images are scaled\n",
    "    # up such that the small side is = IMAGE_MIN_DIM, but ensuring that the\n",
    "    # scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is\n",
    "    # padded with zeros to make it a square so multiple images can be put\n",
    "    # in one batch.\n",
    "    # Available resizing modes:\n",
    "    # none:   No resizing or padding. Return the image unchanged.\n",
    "    # square: Resize and pad with zeros to get a square image\n",
    "    #         of size [max_dim, max_dim].\n",
    "    # pad64:  Pads width and height with zeros to make them multiples of 64.\n",
    "    #         If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales\n",
    "    #         up before padding. IMAGE_MAX_DIM is ignored in this mode.\n",
    "    #         The multiple of 64 is needed to ensure smooth scaling of feature\n",
    "    #         maps up and down the 6 levels of the FPN pyramid (2**6=64).\n",
    "    # crop:   Picks random crops from the image. First, scales the image based\n",
    "    #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of\n",
    "    #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.\n",
    "    #         IMAGE_MAX_DIM is not used in this mode.\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    # Minimum scaling ratio. Checked after MIN_IMAGE_DIM and can force further\n",
    "    # up scaling. For example, if set to 2 then images are scaled up to double\n",
    "    # the width and height, or more, even if MIN_IMAGE_DIM doesn't require it.\n",
    "    # However, in 'square' mode, it can be overruled by IMAGE_MAX_DIM.\n",
    "    IMAGE_MIN_SCALE = 0\n",
    "    # Number of color channels per image. RGB = 3, grayscale = 1, RGB-D = 4\n",
    "    # Changing this requires other changes in the code. See the WIKI for more\n",
    "    # details: https://github.com/matterport/Mask_RCNN/wiki\n",
    "    IMAGE_CHANNEL_COUNT = 3\n",
    "\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "\n",
    "    # Percent of positive ROIs used to train classifier/mask heads\n",
    "    ROI_POSITIVE_RATIO = 0.33\n",
    "\n",
    "    # Pooled ROIs\n",
    "    POOL_SIZE = 7\n",
    "    MASK_POOL_SIZE = 14\n",
    "\n",
    "    # Shape of output mask\n",
    "    # To change this you also need to change the neural network mask branch\n",
    "    MASK_SHAPE = [28, 28]\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 100\n",
    "\n",
    "    # Bounding box refinement standard deviation for RPN and final detections.\n",
    "\n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 35\n",
    "\n",
    "    # Minimum probability value to accept a detected instance\n",
    "    # ROIs below this threshold are skipped\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7\n",
    "\n",
    "    # Non-maximum suppression threshold for detection\n",
    "    DETECTION_NMS_THRESHOLD = 0.3\n",
    "\n",
    "    # Learning rate and momentum\n",
    "    # The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes\n",
    "    # weights to explode. Likely due to differences in optimizer\n",
    "    # implementation.\n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "\n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "\n",
    "    # Loss weights for more precise optimization.\n",
    "    # Can be used for R-CNN training setup.\n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.,\n",
    "        \"rpn_bbox_loss\": 1.,\n",
    "        \"mrcnn_class_loss\": 1.,\n",
    "        \"mrcnn_bbox_loss\": 1.,\n",
    "        \"mrcnn_mask_loss\": 1.\n",
    "    }\n",
    "\n",
    "    # Use RPN ROIs or externally generated ROIs for training\n",
    "    # Keep this True for most situations. Set to False if you want to train\n",
    "    # the head branches on ROI generated by code rather than the ROIs from\n",
    "    # the RPN. For example, to debug the classifier head without having to\n",
    "    # train the RPN.\n",
    "    USE_RPN_ROIS = True\n",
    "\n",
    "    # Train or freeze batch normalization layers\n",
    "    #     None: Train BN layers. This is the normal mode\n",
    "    #     False: Freeze BN layers. Good when using a small batch size\n",
    "    #     True: (don't use). Set layer in training mode even when predicting\n",
    "    TRAIN_BN = False  # Defaulting to False since batch size is often small\n",
    "\n",
    "    # Gradient norm clipping\n",
    "    GRADIENT_CLIP_NORM = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Mask R-CNN model for inference and then load the weights.\n",
    "# This step builds the Keras model architecture.\n",
    "model = mrcnn.model.MaskRCNN(mode=\"inference\", \n",
    "                             config=SimpleConfig(),\n",
    "                             model_dir=os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights into the model.\n",
    "# Download the mask_rcnn_coco.h5 file from this link: https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
    "model.load_weights(filepath=\"./logs/furnitures/furniture_segment.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c154777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input image, convert it from BGR to RGB channel\n",
    "image = cv2.imread(\"../dataset/train/floor_image_22.tiff\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Perform a forward pass of the network to obtain the results\n",
    "r = model.detect([image], verbose=0)\n",
    "\n",
    "# Get the results for the first image.\n",
    "r = r[0]\n",
    "\n",
    "print(image.shape, r['masks'].shape, r['class_ids'].shape, r['rois'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the detected objects.\n",
    "mrcnn.visualize.display_instances(image=image, \n",
    "                                  boxes=r['rois'], \n",
    "                                  masks=r['masks'], \n",
    "                                  class_ids=r['class_ids'], \n",
    "                                  class_names=CLASS_NAMES, \n",
    "                                  scores=r['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7cc158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
