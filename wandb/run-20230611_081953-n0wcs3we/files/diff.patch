diff --git a/final.py b/final.py
index 687b647..37bec01 100644
--- a/final.py
+++ b/final.py
@@ -29,10 +29,13 @@ import cv2
 from mrcnn.visualize import display_instances
 import matplotlib.pyplot as plt
 from tensorflow.keras.callbacks import EarlyStopping
+import wandb
+from wandb.keras import WandbCallback
 
 import warnings
 warnings.filterwarnings("ignore")
 
+wandb.login(key='f5737f3f12d24772aa679fa867d2a7a7efe6664f')
 
 # Root directory of the project
 ROOT_DIR = os.path.abspath("../../")
@@ -52,7 +55,8 @@ DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, "logs")
 ############################################################
 #  Configurations
 ############################################################
-EPOCHS = 1
+EPOCHS = 2
+VAL_STEPS = 10
 
 class CustomConfig(Config):
     """Configuration for training on the dataset.
@@ -69,6 +73,11 @@ class CustomConfig(Config):
     # Number of training steps per epoch
     STEPS_PER_EPOCH = 10
 
+    # Number of validation steps to run at the end of every training epoch.
+    # A bigger number improves accuracy of validation stats, but slows
+    # down the training.
+    VALIDATION_STEPS = VAL_STEPS
+
     BACKBONE = "resnet101"
 
     # Number of classes (including background)
@@ -254,6 +263,18 @@ def train(model):
     # Create an EarlyStopping callback
     early_stopping_callback = EarlyStopping(patience=5, restore_best_weights=True)
 
+    # Create an Wandb Callback
+    wandb_callback = WandbCallback(save_model=True,
+                          save_graph=True,
+                          save_weights_only=True,
+                          log_weights=True,
+                          log_gradients=True,
+                          training_data=dataset_train,
+                          validation_data=dataset_val,
+                          validation_steps = VAL_STEPS,
+                          predictions = 8,
+                          input_type='segmentation_mask')
+
     # *** This training schedule is an example. Update to your needs ***
     # Since we're using a very small dataset, and starting from
     # COCO trained weights, we don't need to train too long. Also,
@@ -263,7 +284,7 @@ def train(model):
                 learning_rate=config.LEARNING_RATE,
                 epochs=EPOCHS,
                 layers='heads',
-                custom_callbacks=[early_stopping_callback])
+                custom_callbacks=[early_stopping_callback, wandb_callback])
 
 
 def color_splash(image, mask):
@@ -378,6 +399,7 @@ if __name__ == '__main__':
     print("Dataset: ", args.dataset)
     print("Logs: ", args.logs)
 
+    config = None
     # Configurations
     if args.command == "train":
         config = CustomConfig()
@@ -390,6 +412,16 @@ if __name__ == '__main__':
         config = InferenceConfig()
     config.display()
 
+    config_dict = config.to_dict()
+    config_dict['Epochs'] = EPOCHS
+
+    myrun = wandb.init(
+                        project='Furniture Segmentation',#project name
+                        group='Test',#set group name
+                        name='Test1',#set run name
+                        resume=False,#resume run
+                        config=config_dict)
+
     # Create model
     if args.command == "train":
         model = modellib.MaskRCNN(mode="training", config=config,
